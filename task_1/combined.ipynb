{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0116a2-0a87-42b7-8df9-b43843f593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium webdriver-manager requests beautifulsoup4 pandas openai\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d940f-714d-4c37-980f-d96ee435504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cf1db-1a0f-4c68-bd20-a073af1f80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Подключение Google Drive (если уже смонтировано, это действие пропускается)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Получаем HTML содержимое страницы\n",
    "url = \"https://www.artrabbit.com/artist-opportunities/\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # проверка на успешный ответ\n",
    "    html_content = response.content\n",
    "    \n",
    "    # Парсим HTML с помощью BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Ищем все элементы с классом artopp\n",
    "    artopp_elements = soup.find_all('div', class_='artopp')\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print(f\"HTTP ошибка: {err}\")\n",
    "except Exception as err:\n",
    "    print(f\"Другая ошибка: {err}\")\n",
    "else:\n",
    "    # Указание заголовков столбцов\n",
    "    headers = ['Data-d', 'Data-a', 'Heading', 'Alert', 'Title', 'Date Updated', 'Body', 'URL']\n",
    "    \n",
    "    # Указываем путь к файлу в Google Drive\n",
    "    file_path = '/content/drive/My Drive/open_calls_ready_2/artist_opportunities_12.csv'\n",
    "    \n",
    "    # Открываем CSV файл для записи\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(headers)  # Записываем заголовки\n",
    "    \n",
    "        # Проходим по каждому элементу artopp и извлекаем данные\n",
    "        for artopp_element in artopp_elements:\n",
    "            try:\n",
    "                # Извлекаем значения атрибутов data-d и data-a\n",
    "                data_d = artopp_element.get('data-d', '')\n",
    "                data_a = artopp_element.get('data-a', '')\n",
    "        \n",
    "                # Извлекаем текстовые поля с проверкой на существование элементов\n",
    "                h3_text = artopp_element.find('h3', class_='b_categorical-heading mod--artopps').text.strip() if artopp_element.find('h3', class_='b_categorical-heading mod--artopps') else ''\n",
    "                p_alert_text = artopp_element.find('p', class_='b_ending-alert mod--just-opened').text.strip() if artopp_element.find('p', class_='b_ending-alert mod--just-opened') else ''\n",
    "                h2_text = artopp_element.find('h2').text.strip() if artopp_element.find('h2') else ''\n",
    "                p_date_text = artopp_element.find('p', class_='b_date').text.strip() if artopp_element.find('p', class_='b_date') else ''\n",
    "                main_body_text = artopp_element.find('div', class_='m_body-copy').text.strip() if artopp_element.find('div', class_='m_body-copy') else ''\n",
    "        \n",
    "                # Извлечение URL из элемента <a>\n",
    "                url_element = artopp_element.find('a', class_='b_submit mod--next')\n",
    "                url = url_element.get('href') if url_element else ''\n",
    "        \n",
    "                # Подготовка данных для записи в CSV\n",
    "                row = [data_d, data_a, h3_text, p_alert_text, h2_text, p_date_text, main_body_text, url]\n",
    "        \n",
    "                # Записываем строку в CSV файл\n",
    "                csvwriter.writerow(row)\n",
    "            except Exception as err:\n",
    "                print(f\"Ошибка парсинга: {err}\")\n",
    "                continue\n",
    "            \n",
    "    print(f\"Файл сохранен по пути: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c11986-a5fc-4cd4-84f8-d41cb1e6b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Настройка опций для Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--remote-debugging-port=9222')\n",
    "chrome_options.add_argument('--user-data-dir=/tmp/user-data')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "\n",
    "# Подключение Google Drive (если уже смонтировано, это действие пропускается)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Указываем путь к файлу в Google Drive\n",
    "file_path = '/content/drive/My Drive/artist_opportunities.csv'\n",
    "\n",
    "# Чтение существующего CSV файла\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Добавление нового столбца для полного текста\n",
    "df['Full Text'] = \"\"\n",
    "\n",
    "# Настройка Selenium WebDriver с использованием webdriver-manager\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Проход по каждой ссылке в CSV и извлечение полного текста\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    if pd.notna(url) and url:  # Проверка на наличие URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Явное ожидание, пока не появится элемент <body> и не загрузится текст\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            full_text = driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
    "        except:\n",
    "            full_text = \"Could not load content\"\n",
    "\n",
    "        df.at[index, 'Full Text'] = full_text\n",
    "\n",
    "# Закрываем браузер\n",
    "driver.quit()\n",
    "\n",
    "# Сохранение обновленного CSV файла\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Файл обновлен и сохранен по пути: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e838a6-2de1-481c-87a9-7c3553bdafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Указываем URL для первой страницы\n",
    "base_url = \"https://www.transartists.org/en/call-artists?page=\"\n",
    "\n",
    "# Указываем путь к файлу CSV в Google Drive (или локальный путь)\n",
    "csv_file_path = '/content/drive/My Drive/open_calls_ready_2/transartists_calls_12.csv'\n",
    "\n",
    "# Функция для распаковки обфусцированного email\n",
    "def decode_spamspan(spamspan_element):\n",
    "    email_parts = spamspan_element.find_all('span')\n",
    "    email = ''.join(part.get_text(strip=True) for part in email_parts)\n",
    "    return email.replace('[at]', '@').replace('[dot]', '.')\n",
    "\n",
    "# Открываем файл для записи данных\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Записываем заголовки столбцов\n",
    "    writer.writerow(['Date', 'Title', 'Description', 'Email', 'Website'])\n",
    "\n",
    "    # Проходим по страницам от 0 до 8\n",
    "    for page_number in range(9):\n",
    "        url = f\"{base_url}{page_number}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # проверка на успешный ответ\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Ищем все элементы <tr> на странице\n",
    "            rows = soup.find_all('tr')\n",
    "    \n",
    "            for row in rows:\n",
    "                # Извлекаем дату\n",
    "                date = row.find('td', class_='views-field views-field-created').get_text(strip=True) if row.find('td', class_='views-field views-field-created') else ''\n",
    "    \n",
    "                # Извлекаем заголовок и описание\n",
    "                content_td = row.find('td', class_='views-field views-field-field-your-ad')\n",
    "                if content_td:\n",
    "                    title = content_td.find('h2').get_text(strip=True) if content_td.find('h2') else ''\n",
    "                    description = content_td.find_all('p')\n",
    "                    description_text = ' '.join([p.get_text(strip=True) for p in description])\n",
    "    \n",
    "                    # Извлечение email\n",
    "                    spamspan_element = content_td.find('a', class_='spamspan')\n",
    "                    email = decode_spamspan(spamspan_element) if spamspan_element else ''\n",
    "    \n",
    "                    # Извлечение веб-сайта\n",
    "                    website = content_td.find('a', href=lambda href: href and \"http\" in href).get('href') if content_td.find('a', href=lambda href: href and \"http\" in href) else ''\n",
    "    \n",
    "                    # Записываем данные в CSV-файл\n",
    "                    writer.writerow([date, title, description_text, email, website])\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(f\"HTTP ошибка: {err}\")\n",
    "            continue\n",
    "        except Exception as err:\n",
    "            print(f\"Другая ошибка: {err}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Данные успешно сохранены в {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562568cb-659b-45c9-8fd7-71f1bef605a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL первой страницы\n",
    "base_url = \"https://resartis.org/open-calls/\"\n",
    "\n",
    "# Путь к файлу CSV на Google Drive (или локальный путь)\n",
    "csv_file_path = '/content/drive/My Drive/open_calls_ready/resartis_calls.csv'\n",
    "\n",
    "# Открываем файл для записи данных\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Записываем заголовки столбцов\n",
    "    writer.writerow([\n",
    "        'Title', 'Deadline', 'Country', 'Description', 'Duration', 'Accommodation',\n",
    "        'Disciplines', 'Studio/Workspace', 'Fees', 'Expectations',\n",
    "        'Application Information', 'Application Deadline',\n",
    "        'Residency Starts', 'Residency Ends', 'Location', 'Link'\n",
    "    ])\n",
    "\n",
    "    # Загружаем основную страницу\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()  # проверка на успешный ответ\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Ищем все элементы с классом grid__item postcard\n",
    "        items = soup.find_all('div', class_='grid__item postcard')\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP ошибка: {err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Другая ошибка: {err}\")\n",
    "    else:            \n",
    "        for item in items:\n",
    "            try:\n",
    "                # Извлекаем заголовок и ссылку на страницу\n",
    "                link_tag = item.find('a', href=True)\n",
    "                link = link_tag['href']\n",
    "                title = item.find('h2', class_='card__title').get_text(strip=True)\n",
    "        \n",
    "                # Печать ссылки и заголовка для проверки\n",
    "                print(f\"Title: {title}\")\n",
    "                print(f\"Link: {link}\")\n",
    "        \n",
    "                # Переход по ссылке и парсинг страницы\n",
    "                detail_response = requests.get(link)\n",
    "                response.raise_for_status()  # проверка на успешный ответ\n",
    "                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
    "        \n",
    "                # Извлечение данных со страницы\n",
    "                description = detail_soup.find('div', class_='entry-content').get_text(strip=True)\n",
    "                deadline = item.find('dt').get_text(strip=True).replace('Deadline: ', '')\n",
    "                country = deadline.split('Country: ')[1] if 'Country: ' in deadline else ''\n",
    "                deadline = deadline.split('Country: ')[0] if 'Country: ' in deadline else deadline\n",
    "        \n",
    "                duration = detail_soup.find('h5', text='Duration of residency').find_next('span').get_text(strip=True)\n",
    "                accommodation = detail_soup.find('h5', text='Accommodation').find_next('span').get_text(strip=True)\n",
    "                disciplines = detail_soup.find('h5', text='Disciplines, work equipment and assistance').find_next('span').get_text(strip=True)\n",
    "                studio = detail_soup.find('h5', text='Studio / Workspace').find_next('span').get_text(strip=True)\n",
    "                fees = detail_soup.find('h5', text='Fees and support').find_next('span').get_text(strip=True)\n",
    "                expectations = detail_soup.find('h5', text='Expectations towards the artist').find_next('span').get_text(strip=True)\n",
    "                application_info = detail_soup.find('h5', text='Application information').find_next('span').get_text(strip=True)\n",
    "        \n",
    "                application_deadline = detail_soup.find('h5', text='Application deadline').find_next('span').get_text(strip=True)\n",
    "                residency_starts = detail_soup.find('h5', text='Residency starts').find_next('span').get_text(strip=True)\n",
    "                residency_ends = detail_soup.find('h5', text='Residency ends').find_next('span').get_text(strip=True)\n",
    "                location = detail_soup.find('h5', text='Location').find_next('span').get_text(strip=True)\n",
    "        \n",
    "                more_info_link = detail_soup.find('h5', text='Link to more information').find_next('a', href=True)['href']\n",
    "        \n",
    "                # Записываем данные в CSV-файл\n",
    "                writer.writerow([\n",
    "                    title, deadline, country, description, duration, accommodation,\n",
    "                    disciplines, studio, fees, expectations, application_info,\n",
    "                    application_deadline, residency_starts, residency_ends, location, more_info_link\n",
    "                ])\n",
    "            except Exception as err:\n",
    "                print(f\"Ошибка парсинга: {err}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Данные успешно сохранены в {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e90d3-fea7-443d-b22a-cd715d104ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# URL страницы\n",
    "url = \"https://resartis.org/open-calls/\"\n",
    "\n",
    "# Заголовки для имитации запроса от браузера\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "# Попробуем загрузить страницу с заголовками и задержкой\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # проверка на успешный ответ\n",
    "    # Парсим HTML-контент\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Выводим HTML-код страницы\n",
    "    print(soup.prettify())\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print(f\"HTTP ошибка: {err}\")\n",
    "except Exception as err:\n",
    "    print(f\"Другая ошибка: {err}\")\n",
    "\n",
    "# Добавляем задержку между запросами\n",
    "time.sleep(5)  # задержка в 5 секунд\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f164b6-40be-42f4-a48d-53ae6d3779c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "# URL страницы\n",
    "base_url = \"https://artistcommunities.org/directory/open-calls\"\n",
    "\n",
    "# Путь к файлу CSV на Google Drive (или локальный путь)\n",
    "csv_file_path = '/content/drive/My Drive/open_calls_ready_2/artist_communities_open_calls_12.csv'\n",
    "\n",
    "# Функция для извлечения текста из блока с проверкой наличия элемента\n",
    "def get_text_or_none(element):\n",
    "    return element.get_text(strip=True) if element else 'N/A'\n",
    "\n",
    "# Открываем файл для записи данных\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Записываем заголовки столбцов\n",
    "    writer.writerow([\n",
    "        'Title', 'Associated Residency Program', 'Organization', 'Description',\n",
    "        'Deadline', 'Application URL', 'Residency Length', 'Languages',\n",
    "        'Average Number of Artists', 'Collaborative Residency', 'Disciplines',\n",
    "        'Companions', 'Country of Residence', 'Family Friendly', 'Stage of Career',\n",
    "        'Additional Expectations', 'Accessible Housing', 'Meals Provided',\n",
    "        'Studios/Special Equipment', 'Studios/Facilities Accessibility',\n",
    "        'Type of Housing', 'Additional Eligibility Information',\n",
    "        'Number of Artists Accepted', 'Total Applicant Pool', 'Artist Stipend',\n",
    "        'Travel Stipend', 'Residency Fees', 'Grant/Scholarship Support',\n",
    "        'Application Fee', 'Application Type'\n",
    "    ])\n",
    "\n",
    "    # Выполняем запрос к базовой странице\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()  # проверка на успешный ответ\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # Ищем все ссылки на страницы опен-коллов\n",
    "        links = soup.select('td.views-field-label a')\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP ошибка: {err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Другая ошибка: {err}\")\n",
    "    else:\n",
    "        for link in links:\n",
    "            try:\n",
    "                # Переход по ссылке на страницу опен-колла\n",
    "                details_url = \"https://artistcommunities.org\" + link['href']\n",
    "                details_response = requests.get(details_url)\n",
    "                details_soup = BeautifulSoup(details_response.content, 'html.parser')\n",
    "    \n",
    "                # Извлечение заголовка\n",
    "                title = get_text_or_none(details_soup.select_one('h1'))\n",
    "    \n",
    "                # Извлечение содержимого блока node__content\n",
    "                content = details_soup.select_one('.node__content')\n",
    "    \n",
    "                associated_residency = get_text_or_none(content.select_one('.field--name-field-associated-residency .field__item a'))\n",
    "                organization = get_text_or_none(content.select_one('.field-pseudo-field--pseudo-group_node\\:organization-link-list .field__item a'))\n",
    "                description = get_text_or_none(content.select_one('.field--name-field-oc-residency-description .field__item'))\n",
    "    \n",
    "                deadline = get_text_or_none(content.select_one('.field--name-field-deadline .datetime'))\n",
    "                application_url = get_text_or_none(content.select_one('.field--name-field-application-url .field__item a'))\n",
    "    \n",
    "                residency_length = get_text_or_none(content.select_one('.field--label-inline.field-pseudo-field--pseudo-residency-length .field__item'))\n",
    "                languages = get_text_or_none(content.select_one('.field--name-field-languages .field__item'))\n",
    "                avg_num_artists = get_text_or_none(content.select_one('.field--name-field-average-artists .field__item'))\n",
    "                collaborative_residency = get_text_or_none(content.select_one('.field--name-field-collaborative-residency .field__item'))\n",
    "                disciplines = ', '.join([item.get_text(strip=True) for item in content.select('.field--name-field-discipline .field__item')])\n",
    "    \n",
    "                companions = get_text_or_none(content.select_one('.field--name-field-companions .field__item'))\n",
    "                country_of_residence = get_text_or_none(content.select_one('.field--name-field-country-of-residence .field__item'))\n",
    "                family_friendly = get_text_or_none(content.select_one('.field--name-field-family-friendly .field__item'))\n",
    "                stage_of_career = get_text_or_none(content.select_one('.field--name-field-stage-of-career .field__item'))\n",
    "                additional_expectations = get_text_or_none(content.select_one('.field--name-field-additional-expectations .field__item'))\n",
    "    \n",
    "                accessible_housing = get_text_or_none(content.select_one('.field--name-field-accessible-housing .field__item'))\n",
    "                meals_provided = ', '.join([item.get_text(strip=True) for item in content.select('.field--name-field-meals-provided .field__item')])\n",
    "                studios_equipment = ', '.join([item.get_text(strip=True) for item in content.select('.field--name-field-studios-special-equipment .field__item')])\n",
    "                studios_accessibility = get_text_or_none(content.select_one('.field--name-field-studios-accessibility .field__item'))\n",
    "                type_of_housing = get_text_or_none(content.select_one('.field--name-field-type-of-housing .field__item'))\n",
    "    \n",
    "                additional_eligibility = get_text_or_none(content.select_one('.field--name-field-additional-eligibility .field__item'))\n",
    "                num_artists_accepted = get_text_or_none(content.select_one('.field--name-field-number-of-artists-accepted .field__item'))\n",
    "                total_applicant_pool = get_text_or_none(content.select_one('.field--name-field-applicant-pool .field__item'))\n",
    "    \n",
    "                artist_stipend = get_text_or_none(content.select_one('.field--name-field-artist-stipend .field__item'))\n",
    "                travel_stipend = get_text_or_none(content.select_one('.field--name-field-travel-stipend .field__item'))\n",
    "                residency_fees = get_text_or_none(content.select_one('.field--name-field-residency-fees .field__item'))\n",
    "                grant_scholarship_support = get_text_or_none(content.select_one('.field--name-field-grant-scholarship .field__item'))\n",
    "                application_fee = get_text_or_none(content.select_one('.field--name-field-application-fee .field__item'))\n",
    "                application_type = get_text_or_none(content.select_one('.field--name-field-application-type .field__item'))\n",
    "    \n",
    "                # Записываем данные в CSV-файл\n",
    "                writer.writerow([\n",
    "                    title, associated_residency, organization, description, deadline, application_url,\n",
    "                    residency_length, languages, avg_num_artists, collaborative_residency, disciplines,\n",
    "                    companions, country_of_residence, family_friendly, stage_of_career, additional_expectations,\n",
    "                    accessible_housing, meals_provided, studios_equipment, studios_accessibility, type_of_housing,\n",
    "                    additional_eligibility, num_artists_accepted, total_applicant_pool, artist_stipend,\n",
    "                    travel_stipend, residency_fees, grant_scholarship_support, application_fee, application_type\n",
    "                ])\n",
    "                print(f\"Processed: {title}\")\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {link['href']}: {e}\")\n",
    "                continue  # Переходим к следующему опен-коллу в случае ошибки\n",
    "\n",
    "        print(f\"Данные успешно сохранены в {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f98e74-7784-4215-9d38-da6475b01d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Путь к вашему CSV файлу с ссылками на Google Drive\n",
    "file_path = '/content/drive/MyDrive/open_calls_ready_2/links/links_artist_callforentry_12.csv'\n",
    "\n",
    "# Загрузка ссылок из CSV файла\n",
    "links_df = pd.read_csv(file_path)\n",
    "links = links_df['Link1'].tolist()  # Предполагается, что столбец называется 'Links'\n",
    "data = []\n",
    "\n",
    "for link in links:\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        response.raise_for_status()  # проверка на успешный ответ\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # Пример парсинга конкретных данных\n",
    "        fair_name = soup.find('div', class_='fairname').text.strip() if soup.find('div', class_='fairname') else ''\n",
    "        print(fair_name)\n",
    "        # Исправленный поиск email\n",
    "        contact_email_tag = soup.find('a', href=True, string=lambda x: x and '@' in x)\n",
    "        contact_email = contact_email_tag.text.strip() if contact_email_tag else ''\n",
    "    \n",
    "        entry_deadline = soup.find('strong', string='Entry Deadline').next_sibling.strip() if soup.find('strong', string='Entry Deadline') else ''\n",
    "        entry_fee = soup.find('strong', string='Entry Fee').next_sibling.strip() if soup.find('strong', string='Entry Fee') else ''\n",
    "    \n",
    "        # Извлечение всего текста из нужных блоков\n",
    "        all_text_blocks = soup.find_all(class_='printable')\n",
    "        all_text = \" \".join([block.get_text(separator=\" \", strip=True) for block in all_text_blocks])\n",
    "    \n",
    "        # Добавление данных в список\n",
    "        data.append({\n",
    "            'Fair Name': fair_name,\n",
    "            'Contact Email': contact_email,\n",
    "            'Entry Deadline': entry_deadline,\n",
    "            'Entry Fee': entry_fee,\n",
    "            'Full Text': all_text\n",
    "        })\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP ошибка: {err}\")\n",
    "        continue\n",
    "    except Exception as err:\n",
    "        print(f\"Другая ошибка: {err}\")\n",
    "        continue\n",
    "\n",
    "# Преобразование списка в DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "output_file_path = '/content/drive/MyDrive/open_calls_ready_2/parsed_artist_callforentry_12.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea3c967-b364-428d-828c-3417f8ec1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Путь к вашему CSV файлу с ссылками на Google Drive\n",
    "file_path = '/content/drive/MyDrive/open_calls_ready_2/links_artconnect.csv'\n",
    "\n",
    "# Загрузка ссылок из CSV файла\n",
    "links_df = pd.read_csv(file_path)\n",
    "links = links_df['Link1'].tolist()  # Предполагается, что столбец называется 'Links'\n",
    "data = []\n",
    "\n",
    "for link in links:\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        response.raise_for_status()  # проверка на успешный ответ\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP ошибка: {err}\")\n",
    "        continue\n",
    "    except Exception as err:\n",
    "        print(f\"Другая ошибка: {err}\")\n",
    "        continue\n",
    "    else:\n",
    "        # Пример извлечения различных данных\n",
    "        try:\n",
    "            title = soup.find('h1', class_='block').text.strip()\n",
    "            print(title)\n",
    "        except AttributeError:\n",
    "            title = ''\n",
    "    \n",
    "        try:\n",
    "            deadline = soup.find('div', class_='text-xs font-normal text-gray-400 mb-1', string='Deadline:').find_next('div').text.strip()\n",
    "        except AttributeError:\n",
    "            deadline = ''\n",
    "    \n",
    "        try:\n",
    "            rewards = soup.find('div', class_='text-xs font-normal text-gray-400 mb-1', string='Rewards:').find_next('div').text.strip()\n",
    "        except AttributeError:\n",
    "            rewards = ''\n",
    "    \n",
    "        try:\n",
    "            fees = soup.find('div', class_='text-xs font-normal text-gray-400 mb-1', string='Fees:').find_next('div').text.strip()\n",
    "        except AttributeError:\n",
    "            fees = ''\n",
    "    \n",
    "        try:\n",
    "            artistic_fields = \", \".join([a.text.strip() for a in soup.find_all('a', class_='text-black-500')])\n",
    "        except AttributeError:\n",
    "            artistic_fields = ''\n",
    "    \n",
    "        try:\n",
    "            overview = soup.find('section', class_='text-base font-normal lg:text-sm').get_text(separator=\" \", strip=True)\n",
    "        except AttributeError:\n",
    "            overview = ''\n",
    "    \n",
    "        # Исправленный поиск контактного email\n",
    "        try:\n",
    "            contact_email = soup.find('a', href=lambda x: x and 'mailto:' in x)['href'].replace('mailto:', '').strip()\n",
    "        except (TypeError, AttributeError):\n",
    "            contact_email = ''\n",
    "    \n",
    "        # Поиск и извлечение ссылки на сайт\n",
    "        try:\n",
    "            website = soup.find('a', href=True, string='Website')['href'].strip()\n",
    "        except (TypeError, AttributeError):\n",
    "            website = ''\n",
    "    \n",
    "        # Добавление всех данных в список\n",
    "        data.append({\n",
    "            'Title': title,\n",
    "            'Deadline': deadline,\n",
    "            'Rewards': rewards,\n",
    "            'Fees': fees,\n",
    "            'Artistic Fields': artistic_fields,\n",
    "            'Overview': overview,\n",
    "            'Contact Email': contact_email,\n",
    "            'Website': website\n",
    "        })\n",
    "\n",
    "# Преобразование списка в DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "output_file_path = '/content/drive/MyDrive/open_calls_ready_2/parsed_links_artconnect.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a2661-bd18-45b3-99d6-536d0ee78898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://www.artjobs.com/open-calls/call-for-artists?page=\"\n",
    "links = []\n",
    "\n",
    "# Проход по страницам с 1 по 25\n",
    "for page in range(1, 26):\n",
    "    url = base_url + str(page)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # проверка на успешный ответ\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # Поиск всех ссылок внутри элемента <tbody>\n",
    "        tbody = soup.find('tbody')\n",
    "        if tbody:\n",
    "            rows = tbody.find_all('tr')\n",
    "            for row in rows:\n",
    "                link_tag = row.find('a', href=True)\n",
    "                if link_tag:\n",
    "                    link = \"https://www.artjobs.com\" + link_tag['href']\n",
    "                    links.append(link)\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP ошибка: {err}\")\n",
    "        continue\n",
    "    except Exception as err:\n",
    "        print(f\"Другая ошибка: {err}\")\n",
    "        continue\n",
    "\n",
    "# Сохранение всех ссылок в CSV\n",
    "df = pd.DataFrame(links, columns=[\"Link\"])\n",
    "output_file_path = '/content/drive/MyDrive/open_calls_ready_2/links_artjobs.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Собрано {len(links)} ссылок и сохранено в {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33df93-44cc-467a-aa50-323eea252972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://www.artjobs.com/open-calls/call-for-artists\"\n",
    "links = []\n",
    "\n",
    "try:\n",
    "    response = requests.get(base_url)\n",
    "    response.raise_for_status()  # проверка на успешный ответ\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Поиск всех ссылок внутри элемента <tbody>\n",
    "    tbody = soup.find('tbody')\n",
    "    if tbody:\n",
    "        rows = tbody.find_all('tr')\n",
    "        for row in rows:\n",
    "            link_tag = row.find('a', href=True)\n",
    "            if link_tag:\n",
    "                link = \"https://www.artjobs.com\" + link_tag['href']\n",
    "                links.append(link)\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print(f\"HTTP ошибка: {err}\")\n",
    "except Exception as err:\n",
    "    print(f\"Другая ошибка: {err}\")\n",
    "    \n",
    "# Сохранение всех ссылок в CSV\n",
    "df = pd.DataFrame(links, columns=[\"Link\"])\n",
    "output_file_path = '/content/drive/MyDrive/open_calls_ready_2/links_artjobs_1.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Собрано {len(links)} ссылок и сохранено в {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786117e-ae10-4e66-aa7b-c9cbd70fbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Убедитесь, что файл с ссылками уже существует и загружен\n",
    "links_file_path = '/content/drive/MyDrive/open_calls_ready_2/links_artjobs.csv'\n",
    "\n",
    "# Загрузка списка ссылок\n",
    "try:\n",
    "    links_df = pd.read_csv(links_file_path)\n",
    "    links = links_df['Link'].tolist()\n",
    "except Exception as err:\n",
    "    print(f\"Другая ошибка: {err}\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Проход по всем ссылкам\n",
    "for link in links:\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        response.raise_for_status()  # проверка на успешный ответ\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP ошибка: {err}\")\n",
    "        continue\n",
    "    except Exception as err:\n",
    "        print(f\"Другая ошибка: {err}\")\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            title = soup.find('h1', class_='title').text.strip()\n",
    "            print(title)\n",
    "        except AttributeError:\n",
    "            title = ''\n",
    "    \n",
    "        try:\n",
    "            call_type = soup.find('div', class_='field-name-field-open-call-type').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            call_type = ''\n",
    "    \n",
    "        try:\n",
    "            industry = soup.find('div', class_='field-name-field-opencall-industry').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            industry = ''\n",
    "    \n",
    "        try:\n",
    "            category = soup.find('div', class_='field-name-field-category-addapost').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            category = ''\n",
    "    \n",
    "        try:\n",
    "            theme = soup.find('div', class_='field-name-field-open-call-theme').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            theme = ''\n",
    "    \n",
    "        try:\n",
    "            country = soup.find('div', class_='field-name-field-tags-news-country').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            country = ''\n",
    "    \n",
    "        try:\n",
    "            organisation = soup.find('div', class_='field-name-field-organisation').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            organisation = ''\n",
    "    \n",
    "        try:\n",
    "            eligibility = soup.find('div', class_='field-name-field-eligibility').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            eligibility = ''\n",
    "    \n",
    "        try:\n",
    "            keywords = soup.find('div', class_='field-name-field-tags-news').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            keywords = ''\n",
    "    \n",
    "        try:\n",
    "            description = soup.find('div', class_='field-name-field-description').get_text(separator=' ', strip=True)\n",
    "        except AttributeError:\n",
    "            description = ''\n",
    "    \n",
    "        try:\n",
    "            prize_summary = soup.find('div', class_='field-name-field-prize-summary').get_text(separator=' ', strip=True)\n",
    "        except AttributeError:\n",
    "            prize_summary = ''\n",
    "    \n",
    "        try:\n",
    "            prizes_details = soup.find('div', class_='field-name-field-opencall-prizes').get_text(separator=' ', strip=True)\n",
    "        except AttributeError:\n",
    "            prizes_details = ''\n",
    "    \n",
    "        try:\n",
    "            event_date = soup.find('div', class_='field-name-field-opencall-event-date').get_text(separator=' ', strip=True)\n",
    "        except AttributeError:\n",
    "            event_date = ''\n",
    "    \n",
    "        try:\n",
    "            deadline = soup.find('div', class_='field-name-field-deadline-data').get_text(separator=' ', strip=True)\n",
    "        except AttributeError:\n",
    "            deadline = ''\n",
    "    \n",
    "        try:\n",
    "            entry_fee = soup.find('div', class_='field-name-field-entry-fee').find('ul').text.strip()\n",
    "        except AttributeError:\n",
    "            entry_fee = ''\n",
    "    \n",
    "        try:\n",
    "            fee_detail = soup.find('div', class_='field-name-field-application-fee').get_text(separator=' ', strip=True)\n",
    "        except AttributeError:\n",
    "            fee_detail = ''\n",
    "    \n",
    "        try:\n",
    "            contact_links = soup.find('div', class_='field-name-field-post-contact-links').get_text(separator=' ', strip=True)\n",
    "        except AttributeError:\n",
    "            contact_links = ''\n",
    "    \n",
    "        try:\n",
    "            instagram = soup.find('div', class_='field-name-field-opencall-instagram').find('a')['href']\n",
    "        except AttributeError:\n",
    "            instagram = ''\n",
    "    \n",
    "        # Добавление всех данных в список\n",
    "        data.append({\n",
    "            'Title': title,\n",
    "            'Call Type': call_type,\n",
    "            'Industry': industry,\n",
    "            'Category': category,\n",
    "            'Theme': theme,\n",
    "            'Country': country,\n",
    "            'Organisation': organisation,\n",
    "            'Eligibility': eligibility,\n",
    "            'Keywords': keywords,\n",
    "            'Description': description,\n",
    "            'Prize Summary': prize_summary,\n",
    "            'Prizes Details': prizes_details,\n",
    "            'Event Date': event_date,\n",
    "            'Deadline': deadline,\n",
    "            'Entry Fee': entry_fee,\n",
    "            'Fee Detail': fee_detail,\n",
    "            'Contact & Links': contact_links,\n",
    "            'Instagram': instagram\n",
    "        })\n",
    "\n",
    "# Преобразование списка в DataFrame и сохранение в CSV\n",
    "df = pd.DataFrame(data)\n",
    "output_file_path = '/content/drive/MyDrive/open_calls_ready_2/parsed_artjobs_details.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Собрано {len(data)} записей и сохранено в {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11ea37-2f09-4ecd-ab62-1ab819a68464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Подключение Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Настройте ваш API ключ OpenAI\n",
    "openai.api_key = 'KEY'\n",
    "\n",
    "def ask_openai(question, prompt_prefix=\"\"):\n",
    "    \"\"\"Задает вопрос OpenAI и возвращает ответ.\"\"\"\n",
    "    prompt = f\"{prompt_prefix}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=4000,\n",
    "            temperature=1\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обращении к OpenAI: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def process_csv_files(directory_path):\n",
    "    \"\"\"Обрабатывает все CSV файлы в папке, задает вопросы OpenAI и возвращает результаты.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"Файл {file_path} успешно загружен.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при загрузке файла {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                # Объединяем данные из строки с заголовками\n",
    "                data = \" \".join([f\"{col}: {str(value)}\" for col, value in row.items()])\n",
    "\n",
    "                # Задаем вопросы OpenAI\n",
    "                city_country = ask_openai(f\"Верни на английском языке ТОЛЬКО страну, если указано. Use UK for United Kingdom and USA for United States and full name for other countries. Данные: {data}. Если информации нет, то напиши Go to the application page for details.\")\n",
    "                print(city_country)\n",
    "                opencall_title = ask_openai(f\"Верни на английском языке ТОЛЬКО название опен-колла. Оно может содержать название галереи, выставки, ярмарки и т.д. Данные: {data}. Если информации нет, то напиши Go to the application page for details.\")\n",
    "                print(opencall_title)\n",
    "                deadline_date = ask_openai(f\"Верни на английском языке ТОЛЬКО дату дедлайна в формате YYYY-MM-DD. Данные: {data}. Если информации нет, то напиши 2024-10-30.\")\n",
    "                print(deadline_date)\n",
    "                event_date = ask_openai(f\"Верни на английском языке ТОЛЬКО дату самого мероприятия (не дату дедлайна) в формате YYYY-MM-DD. Мероприятие всегда позже, чем дата дедлайна. Данные: {data}. Если информации нет, то напиши 2024-10-30.\")\n",
    "                print(event_date)\n",
    "                application_form_link = ask_openai(f\"Верни на английском языке ТОЛЬКО ссылку на форму для подачи заявки. Обычно она находится в графе Website, Application Link, URL. Данные: {data}. Если информации нет, то напиши Go to the application page for details.\")\n",
    "                print(application_form_link)\n",
    "                selection_criteria = ask_openai(f\"Верни на английском языке ТОЛЬКО критерии отбора художников и работ. Данные: {data}. Если информации нет, то напиши Go to the application page for details.\")\n",
    "                print(selection_criteria)\n",
    "                fee = ask_openai(f\"Верни на английском языке ТОЛЬКО стоимость участия (не награду, а стоимость участия).Identify the participation fee based on the following information. Return only fee and nothing more. If there is 'no' or 'no fee' return 'no fee'. Without your thoughts. Данные: {data}. Если информации нет, то напиши Go to the application page for details.\")\n",
    "                print(fee)\n",
    "\n",
    "                faq = ask_openai(\n",
    "                    f\"Составь на английском языке ТОЛЬКО FAQ по следующему формату (ЭТО ПРИМЕРНЫЙ ФОРМАТ, ТЫ МОЖЕШЬ ДОБАВЛЯТЬ ИЛИ УБИРАТЬ ПУНКТЫ): \\n\"\n",
    "                    \"Who is eligible for this opportunity?: \\n\"\n",
    "                    \"When is the deadline?: \\n\"\n",
    "                    \"How many works can I submit?: \\n\"\n",
    "                    \"When is the delivery date?: \\n\"\n",
    "                    \"When do I need to collect my work?: \\n\"\n",
    "                    \"How much does it cost?: \\n\"\n",
    "                    \"Are there payments to artists?: \\n\"\n",
    "                    \"How do you decide on proposals?: \\n\"\n",
    "                    \"What happens if my proposal is chosen?:\\n\"\n",
    "                    \"What kind of proposals are you looking for?: \\n\"\n",
    "                    \"Where is the [OPPORTUNITY NAME] held?: \\n\"\n",
    "                    \"Why we do it: \\n\"\n",
    "                    f\"Данные: {data}. Если информации нет, то напиши Go to the application page for details.\"\n",
    "                )\n",
    "                print(faq)\n",
    "\n",
    "                application_guide = ask_openai(f\"Верни на английском языке ТОЛЬКО подробный и написанный простыми словами план для художника, как податься на опен-колл. Без воды и банальностей, а только полезная инфа, основанная на данных опен-колла и основные шаги из общей практики подачи заявок на опен-колл. Так чтобы художник смог скопировать и вставить в свой список дел. Также ты можешь использовать свои знания о площадке проведения опен-колла для уточнения плана. Данные: {data}. Если информации нет, то напиши Go to the application page for details.\")\n",
    "                print(application_guide)\n",
    "\n",
    "                # Сохраняем результаты\n",
    "                results.append({\n",
    "                    \"City_Country\": city_country,\n",
    "                    \"Open_Call_Title\": opencall_title,\n",
    "                    \"Deadline_Date\": deadline_date,\n",
    "                    \"Event_Date\": event_date,\n",
    "                    \"Application_Form_Link\": application_form_link,\n",
    "                    \"Selection_Criteria\": selection_criteria,\n",
    "                    \"FAQ\": faq,\n",
    "                    \"Application_Guide\": application_guide,\n",
    "                    \"Fee\": fee\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results(results, output_file):\n",
    "    \"\"\"Сохраняет результаты в CSV файл.\"\"\"\n",
    "    try:\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results = df_results.drop_duplicates()\n",
    "        df_results.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Результаты успешно сохранены в {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении файла: {e}\")\n",
    "\n",
    "# Основной процесс\n",
    "directory_path = '/content/drive/MyDrive/open_calls_ready_2'\n",
    "try:\n",
    "    results = process_csv_files(directory_path)\n",
    "    if results:\n",
    "        save_results(results, '/content/drive/MyDrive/open_calls_ready_2/results.csv')\n",
    "    else:\n",
    "        print(\"Нет данных для сохранения.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка в процессе выполнения: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dbc009-7417-404c-8730-e29220295301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Подключение Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "def send_post_request(row):\n",
    "    \"\"\"Отправляет POST-запрос на указанный URL с данными из строки файла.\"\"\"\n",
    "    url = \"https://beta.mirr.art/api/open_calls/\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer ed1beebf3ede45c9a55835b5166c10b5\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"city_country\": row['City_Country'],\n",
    "        \"open_call_title\": row['Open_Call_Title'],\n",
    "        \"deadline_date\": row['Deadline_Date'],\n",
    "        \"event_date\": row['Event_Date'],\n",
    "        \"application_from_link\": row['Application_Form_Link'],\n",
    "        \"selection_criteria\": row['Selection_Criteria'],\n",
    "        \"faq\": row['FAQ'],\n",
    "        \"fee\": row['Fee'],\n",
    "        \"application_guide\": row['Application_Guide'],\n",
    "        \"open_call_description\": f\"Open call in {row['City_Country']} titled {row['Open_Call_Title']}.\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Успешно отправлены данные для Open Call: {row['Open_Call_Title']}\")\n",
    "        else:\n",
    "            print(f\"Ошибка при отправке данных для Open Call: {row['Open_Call_Title']}. Статус код: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при отправке POST-запроса: {e}\")\n",
    "\n",
    "def process_csv_and_send_requests(file_path):\n",
    "    \"\"\"Читает CSV файл и отправляет POST-запросы по каждой строке.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)  # Замените кодировку на 'ISO-8859-1'\n",
    "        print(f\"Файл {file_path} успешно загружен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке файла {file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        send_post_request(row)\n",
    "        print(row['Open_Call_Title'])\n",
    "\n",
    "# Основной процесс\n",
    "csv_file_path = '/content/drive/MyDrive/open_calls_ready_2/open_calls_after_25_11.csv'\n",
    "try:\n",
    "    process_csv_and_send_requests(csv_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка в процессе выполнения: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
